{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/contentlab-io/ai_language_translation/blob/main/NMT7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1clSaPz3RWvz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "!pip install keras_self_attention\r\n",
        "\r\n",
        "#from translate.storage.tmx import tmxfile\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import re\r\n",
        "import array\r\n",
        "\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "import numpy as np\r\n",
        "import string\r\n",
        "from numpy import array, argmax, random, take\r\n",
        "#for processing imported data\r\n",
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "#the RNN routines\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\r\n",
        "\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.models import load_model\r\n",
        "from keras import optimizers\r\n",
        "\r\n",
        "#optional imports if you want to generate statistical graphs of the DMT\r\n",
        "#import matplotlib.pyplot as plt\r\n",
        "#from keras.utils import plot_model\r\n",
        "#import pydot\r\n",
        "\r\n",
        "\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from gensim.test.utils import common_texts\r\n",
        "from keras_self_attention import SeqSelfAttention\r\n",
        "\r\n",
        "# function to read raw text file\r\n",
        "def read_text(filename):\r\n",
        "        # open the file\r\n",
        "        file = open(filename, mode='rt', encoding='utf-8')\r\n",
        "        \r\n",
        "        # read all text\r\n",
        "        text = file.read()\r\n",
        "        file.close()\r\n",
        "        return text\r\n",
        "\r\n",
        "\t\t\r\n",
        "# split a text into sentences\r\n",
        "def to_lines(text):\r\n",
        "      sents = text.strip().split('\\n')\r\n",
        "      sents = [i.split('\\t') for i in sents]\r\n",
        "      return sents\r\n",
        "\r\n",
        "### tokenizer ###\r\n",
        "def tokenization(lines):\r\n",
        "        #print(lines)\r\n",
        "        tokenizer = Tokenizer()\r\n",
        "\r\n",
        "        tokenizer.fit_on_texts(lines)\r\n",
        "        return tokenizer\r\n",
        "\r\n",
        "### encode ###\r\n",
        "def encode_sequences(tokenizer, length, lines):\r\n",
        "         # integer encode sequences\r\n",
        "         seq = tokenizer.texts_to_sequences(lines)\r\n",
        "         # pad sequences with 0 values\r\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\r\n",
        "         return seq\r\n",
        "### custom split train/test ###\r\n",
        "\r\n",
        "\r\n",
        "def split1(lines):\r\n",
        "        train=[]\r\n",
        "        test=[]\r\n",
        "        l=len(lines)\r\n",
        "        for i in range(0,l-1):\r\n",
        "                if (i%8!=0):\r\n",
        "                        test.append(lines[i])\r\n",
        "                else:\r\n",
        "                        train.append(lines[i])\r\n",
        "        return [train,test]\r\n",
        "\r\n",
        "def split_file(fname1,fname2):\r\n",
        "        content_array = []\r\n",
        "        with open(fname1) as f:\r\n",
        "                #Content_list is the list that contains the read lines.     \r\n",
        "                for line in f:\r\n",
        "                        tokens = line.split('\\t')\r\n",
        "                        content_array.append(tokens[0])\r\n",
        "                        \r\n",
        "                with open(fname2, \"w\") as txt_file:\r\n",
        "                    for line in content_array:\r\n",
        "                        txt_file.write(line+\"\\n\")\r\n",
        "\r\n",
        "\t\t\r\n",
        "\r\n",
        "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units,use_attention=1,use_word2vec=1,corpus=None):\r\n",
        "      model = Sequential()\r\n",
        "      if use_word2vec==0 :\r\n",
        "        model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\r\n",
        "      else :\r\n",
        "        model_w2v = Word2Vec(corpus_file=corpus, size=50, window=5, min_count=1, workers=4)\r\n",
        "        model_w2v.mask_zero = True\r\n",
        "        model.add(model_w2v.wv.get_keras_embedding(train_embeddings=True))\r\n",
        "        \r\n",
        "      model.add(LSTM(units))\r\n",
        "      model.add(RepeatVector(out_timesteps))\r\n",
        "\r\n",
        "      if use_attention == 1:\r\n",
        "        model.add(SeqSelfAttention(attention_activation='sigmoid'))\r\n",
        "\r\n",
        "      model.add(LSTM(units, return_sequences=True))\r\n",
        "      model.add(Dense(out_vocab, activation='softmax'))\r\n",
        "      return model\r\n",
        "\r\n",
        "def get_word(n, tokenizer):\r\n",
        "      for word, index in tokenizer.word_index.items():\r\n",
        "          if index == n:\r\n",
        "              return word\r\n",
        "      return None\r\n",
        "\r\n",
        "def train_model(path_to_data,path_to_model,use_attention=1,use_word2vec=1):\r\n",
        "\r\n",
        "  data = read_text(path_to_data)\r\n",
        "  en_ru = to_lines(data)\r\n",
        "  en_ru = array(en_ru)\r\n",
        "\r\n",
        "  #print(en_ru)\r\n",
        "\r\n",
        "  # prepare english tokenizer\r\n",
        "  en_tokenizer = tokenization(en_ru[:, 0])\r\n",
        "  en_vocab_size = len(en_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "  en_length = 8\r\n",
        "  #print('English Vocabulary Size: %d' % en_vocab_size)\r\n",
        "\r\n",
        "  # prepare Russian tokenizer\r\n",
        "  ru_tokenizer = tokenization(en_ru[:, 1])\r\n",
        "  ru_vocab_size = len(ru_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "  ru_length = 8\r\n",
        "  #print('Target Vocabulary Size: %d' % ru_vocab_size)\r\n",
        "\r\n",
        "  from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "  # split data into train and test set\r\n",
        "  train, test = train_test_split(en_ru, test_size=0.2, random_state = 12)\r\n",
        "\r\n",
        "  # prepare training data\r\n",
        "  #input == english\r\n",
        "  trainX = encode_sequences(en_tokenizer, en_length, train[:, 0])\r\n",
        "\r\n",
        "  #output == russian\r\n",
        "  trainY = encode_sequences(ru_tokenizer, ru_length, train[:, 1])\r\n",
        "\r\n",
        "  # prepare validation data\r\n",
        "  #input == english\r\n",
        "  testX = encode_sequences(en_tokenizer, en_length, test[:, 0])\r\n",
        "\r\n",
        "  #output == russian\r\n",
        "  testY = encode_sequences(ru_tokenizer, ru_length, test[:, 1])\r\n",
        "\r\n",
        "  corpus=None\r\n",
        "\r\n",
        "  if use_word2vec==1 :\r\n",
        "    corpus=\"tmp.txt\"\r\n",
        "    split_file(path_to_data,corpus)\r\n",
        "      \r\n",
        "  # model compilation\r\n",
        "  model = define_model(en_vocab_size, ru_vocab_size, en_length, ru_length, 512,use_attention,use_word2vec,corpus)\r\n",
        "  rms = optimizers.RMSprop(lr=0.001)\r\n",
        "  model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\r\n",
        "\r\n",
        "  #filename = 'model13'\r\n",
        "  filename = path_to_model\r\n",
        "\r\n",
        "  checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "\r\n",
        "  with tf.device('/device:GPU:0'):\r\n",
        "    # train model\r\n",
        "    history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\r\n",
        "                      epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \r\n",
        "                      verbose=1)\r\n",
        "            \r\n",
        "#end function\r\n",
        "\r\n",
        "def translate(path_to_data, path_to_file, path_to_model):\r\n",
        "\r\n",
        "  data = read_text(path_to_data)\r\n",
        "  en_ru = to_lines(data)\r\n",
        "  en_ru = array(en_ru)\r\n",
        "\r\n",
        "  #print(en_ru)\r\n",
        "\r\n",
        "  # prepare english tokenizer\r\n",
        "  en_tokenizer = tokenization(en_ru[:, 0])\r\n",
        "  en_vocab_size = len(en_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "  en_length = 8\r\n",
        "  print('English Vocabulary Size: %d' % en_vocab_size)\r\n",
        "\r\n",
        "  # prepare Russian tokenizer\r\n",
        "  ru_tokenizer = tokenization(en_ru[:, 1])\r\n",
        "  ru_vocab_size = len(ru_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "  print('ru_vocab_size Vocabulary Size: %d' % ru_vocab_size)\r\n",
        "\r\n",
        "  data2 = read_text(path_to_file)\r\n",
        "  en_ru2 = to_lines(data2)\r\n",
        "  en_ru2 = array(en_ru2)\r\n",
        "\r\n",
        "  testX1 = encode_sequences(en_tokenizer, en_length, en_ru2[:, 0])\r\n",
        "\r\n",
        "  model = load_model(path_to_model)\r\n",
        "\r\n",
        "  #predict (from english to dutch)\r\n",
        "  preds = model.predict_classes(testX1.reshape((testX1.shape[0],testX1.shape[1])))\r\n",
        "\r\n",
        "  #actuals_text=[]\r\n",
        "  preds_text = []\r\n",
        "  inputs_text=[]\r\n",
        "\r\n",
        "  idx=0\r\n",
        "\r\n",
        "  for i in preds:\r\n",
        "        idx=idx+1\r\n",
        "        temp = []\r\n",
        "        for j in range(len(i)):\r\n",
        "              t = get_word(i[j], ru_tokenizer)\r\n",
        "              if j > 0:\r\n",
        "                  if (t == get_word(i[j-1], ru_tokenizer)) or (t == None):\r\n",
        "                      temp.append('')\r\n",
        "                  else:\r\n",
        "                      temp.append(t)\r\n",
        "              else:\r\n",
        "                    if(t == None):\r\n",
        "                            temp.append('')\r\n",
        "                    else:\r\n",
        "                            temp.append(t)\r\n",
        "        preds_text.append(' '.join(temp))\r\n",
        "      \r\n",
        "  idx=0\r\n",
        "\t   \r\n",
        "  for i in testX1:\r\n",
        "        idx=idx+1\r\n",
        "\r\n",
        "        temp = []\r\n",
        "        for j in range(len(i)):\r\n",
        "              t = get_word(i[j], en_tokenizer)\r\n",
        "              if j > 0:\r\n",
        "                  if (t == get_word(i[j-1], en_tokenizer)) or (t == None):\r\n",
        "                      temp.append('')\r\n",
        "                  else:\r\n",
        "                      temp.append(t)\r\n",
        "              else:\r\n",
        "                    if(t == None):\r\n",
        "                            temp.append('')\r\n",
        "                    else:\r\n",
        "                            temp.append(t)\r\n",
        "        inputs_text.append(' '.join(temp))\r\n",
        "\r\n",
        "  pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\r\n",
        "\r\n",
        "\r\n",
        "  pred_df = pd.DataFrame({'input' : inputs_text , 'model translation' : preds_text})\r\n",
        "\r\n",
        "  print(pred_df)\r\n",
        "\r\n",
        "#end function\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_model(\"fra.txt\",\"model_fra2\",1,1)\r\n",
        "translate(\"fra.txt\",\"test.txt\",\"model_fra2\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}